{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c32ffd4-3013-4a8a-9355-b784f5e4c83c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "595f94c6-d30b-4f9f-b34a-0e106e7df10a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Definición de rutas de almacenamiento para las diferentes capas de procesamiento de datos\n",
    " \n",
    " 1. **landing_path**: Ruta donde se almacenan los datos crudos que se reciben desde las fuentes externas, antes de cualquier transformación o limpieza.\n",
    " 2. **bronze_path**: Ruta donde se almacenan los datos transformados mínimamente, a menudo llamados \"datos de bronce\". En esta capa se realizan las primeras limpiezas y validaciones.\n",
    " 3. **silver_path**: Ruta para almacenar los datos que han sido limpiados y transformados en una forma más estructurada, listos para análisis más complejos.\n",
    " 4. **gold_path**: Ruta donde se almacenan los datos más refinados, listos para ser utilizados en análisis avanzados, visualizaciones o para la toma de decisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e43a438-0af7-443e-a48b-e4c19a789bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "landing_path = '/mnt/project/Landing/'\n",
    "bronze_path = '/mnt/project/Bronze/'\n",
    "silver_path = '/mnt/project/Silver/'\n",
    "gold_path = '/mnt/project/gold/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c75a2054-8b12-4ac7-9a6a-808cbcf06136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "El proposito es construir dinámicamente la cadena de conexión JDBC a una base de datos SQL Server usando valores almacenados de manera segura en Azure Databricks Secrets sto asegura que las credenciales de acceso al servidor y a la base de datos no se expongan directamente en el código, mejorando la seguridad.\n",
    "\n",
    "1.  **Obtención de credenciales de Azure Databricks Secrets:**\n",
    "\n",
    "- `dbutils.secrets.get(scope = \"sc-adls\", key = \"ServerName\"):` Este comando obtiene el valor del secreto **ServerName** desde el almacén de secretos de Azure Databricks con el scope sc-adls. Esto proporciona el nombre del servidor SQL al que se conectará.\n",
    "- `dbutils.secrets.get(scope = \"sc-adls\", key = \"DatabaseName\"):` De manera similar, este comando obtiene el valor del secreto DatabaseName desde el mismo almacén de secretos, proporcionando el nombre de la base de datos a la que se va a acceder.\n",
    "\n",
    "2.  **Creación de la cadena de conexión JDBC:**\n",
    "\n",
    "- `url = \"jdbc:sqlserver://{0};database={1}\".format(server_name, database_name):` Este comando formatea una cadena JDBC para conectar a una base de datos SQL Server, reemplazando `{0}` por el nombre del servidor (**`server_name`**) y `{1}` por el nombre de la base de datos (**`database_name`**). El resultado es una cadena de conexión completa que permite a la aplicación conectarse al servidor SQL y a la base de datos especificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351a125e-fd5e-424b-9991-35ef40c9e4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Azure SQL Server details\n",
    "server_name = dbutils.secrets.get(scope = \"sc-adls\", key = \"ServerName\")\n",
    "database_name = dbutils.secrets.get(scope = \"sc-adls\", key = \"DatabaseName\")\n",
    "url = \"jdbc:sqlserver://{0};database={1}\".format(server_name, database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "accc23eb-9cf0-4928-af49-bd34474474c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "obtener de forma segura las credenciales de usuario (nombre de usuario y contraseña) almacenadas en el almacén de secretos de Azure Databricks.\n",
    "\n",
    "- `user = dbutils.secrets.get(scope = \"sc-adls\", key = \"UserName\"):` Este comando obtiene el valor del secreto UserName desde el almacén de secretos de Azure Databricks, con el scope sc-adls. Este secreto contiene el nombre de usuario que se utilizará para conectarse a la base de datos o a otros recursos protegidos.\n",
    "Obtención de la contraseña:\n",
    "\n",
    "- `password = dbutils.secrets.get(scope = \"sc-adls\", key = \"PasswordDatabase\"):` De manera similar, este comando obtiene el valor del secreto PasswordDatabase desde el mismo almacén de secretos, proporcionando la contraseña asociada con el nombre de usuario previamente recuperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adf9b601-686b-4513-b56a-64173c6a58f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Azure SQL Server authentication details\n",
    "user = dbutils.secrets.get(scope = \"sc-adls\", key = \"UserName\")\n",
    "password = dbutils.secrets.get(scope = \"sc-adls\", key = \"PasswordDatabase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97eaafb2-d3c3-4fb8-b28e-af2d6eb337be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Las consultas se generan dinámicamente, permitiendo leer de manera flexible las tablas Delta desde el directorio `silver_path`, con cada nombre de tabla proporcionado en las variables `statement1`, `statement2`, `statement3` y `statement4`. Este enfoque permite mantener la flexibilidad al ejecutar consultas sobre diferentes conjuntos de datos dentro del mismo entorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2195e8-b44b-4cb6-9527-811e20060d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_1 = \"\"\"select \n",
    "                *\n",
    "           from delta.`{}/{}`\"\"\".format(silver_path,'statement1')\n",
    "\n",
    "query_2 = \"\"\"select \n",
    "                *\n",
    "           from delta.`{}/{}`\"\"\".format(silver_path,'statement2')\n",
    "\n",
    "query_3 = \"\"\"select \n",
    "                *\n",
    "           from delta.`{}/{}`\"\"\".format(silver_path,'statement3')\n",
    "\n",
    "query_4 = \"\"\"select \n",
    "                *\n",
    "           from delta.`{}/{}`\"\"\".format(silver_path,'statement4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "377fc115-2308-4e3c-98bd-bc3f5e5690d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ejecucion de las consultas Creadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e537d8-df56-4509-b23a-f23fa84a763e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "statement1 = spark.sql(query_1)\n",
    "statement2 = spark.sql(query_2)\n",
    "statement3 = spark.sql(query_3)\n",
    "statement4 = spark.sql(query_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567d034c-0eeb-487f-828a-bd88d59f1d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Define los nombres de las Tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fbfb01c-01a5-459c-aa95-1c3e697d795c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the table name\n",
    "table_name_1 = \"statement1\"\n",
    "table_name_2 = \"statement2\"\n",
    "table_name_3 = \"statement3\"\n",
    "table_name_4 = \"statement4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aab12d0b-be7d-48f1-a288-9e2d520f6b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Guardar Datos en Azure SQL Database\n",
    "El siguiente código define una función save_to_azure_sql que guarda un DataFrame de PySpark en una base de datos SQL de Azure. Esta función utiliza el conector JDBC para escribir los datos de manera eficiente en la tabla correspondiente de la base de datos SQL de Azure.\n",
    "\n",
    "Función save_to_azure_sql:\n",
    "La función save_to_azure_sql toma un DataFrame de PySpark y el nombre de la tabla en la base de datos SQL de Azure como argumentos y guarda los datos en dicha tabla. El proceso se realiza mediante la conexión JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e02fae-e105-4030-8306-690fb1fa981d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## SAVE DATA AZURE SQL DATABASE\n",
    "def save_to_azure_sql(df, table_name):\n",
    "    \"\"\"\n",
    "    Saves a PySpark DataFrame to Azure SQL Database with improved efficiency.\n",
    " \n",
    "    Args:\n",
    "        df (pyspark.sql.DataFrame): The DataFrame to save.\n",
    "        table_name (str): The name of the Azure SQL table to write to.\n",
    "    \"\"\"\n",
    " \n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", url) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", user) \\\n",
    "        .option(\"password\", password) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    " \n",
    "# Save each DataFrame using the function\n",
    "save_to_azure_sql(statement1, table_name_1)\n",
    "save_to_azure_sql(statement2, table_name_2)\n",
    "save_to_azure_sql(statement3, table_name_3)\n",
    "save_to_azure_sql(statement4, table_name_4)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}